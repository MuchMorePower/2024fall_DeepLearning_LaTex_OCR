Model(
  (encoder): Encoder(
    (first_conv): Sequential(
      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (downSample): Sequential(
      (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
    (layer0): Sequential(
      (0): CBAMBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (1): CBAMBasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
    )
    (layer1): Sequential(
      (0): CBAMBasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=128, out_features=8, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=8, out_features=128, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (1): CBAMBasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=128, out_features=8, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=8, out_features=128, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (2): CBAMBasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=128, out_features=8, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=8, out_features=128, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (3): CBAMBasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=128, out_features=8, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=8, out_features=128, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (4): CBAMBasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=128, out_features=8, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=8, out_features=128, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (5): CBAMBasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=128, out_features=8, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=8, out_features=128, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
    )
    (layer2): Sequential(
      (0): CBAMBasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (1): CBAMBasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (2): CBAMBasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (3): CBAMBasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (4): CBAMBasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
      (5): CBAMBasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (cbam): CBAM(
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=256, out_features=16, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=16, out_features=256, bias=False)
              (3): Sigmoid()
            )
          )
          (spatial_attention): SpatialAttention(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (sigmoid): Sigmoid()
          )
        )
      )
    )
    (dense_block1): DenseBlock(
      (layers): ModuleList(
        (0): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (1): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (2): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (3): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (4): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (5): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (6): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (7): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (8): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (9): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (10): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (11): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (12): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (13): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (14): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (15): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
      )
    )
    (channel_attention1): ChannelAttention(
      (shared_mlp): Sequential(
        (0): Linear(in_features=768, out_features=48, bias=False)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=48, out_features=768, bias=False)
        (3): Sigmoid()
      )
    )
    (transition1): TransitionLayer(
      (transition): Sequential(
        (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Dropout(p=0.2, inplace=False)
        (4): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block2): DenseBlock(
      (layers): ModuleList(
        (0): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (1): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (2): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (3): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (4): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (5): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (6): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (7): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (8): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (9): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (10): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (11): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (12): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (13): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (14): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
        (15): Bottleneck(
          (bottleneck): Sequential(
            (0): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
          (channel_attention): ChannelAttention(
            (shared_mlp): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
        )
      )
    )
    (channel_attention2): ChannelAttention(
      (shared_mlp): Sequential(
        (0): Linear(in_features=896, out_features=56, bias=False)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=56, out_features=896, bias=False)
        (3): Sigmoid()
      )
    )
    (final_conv): Conv2d(896, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decoder): DecoderWithAttention(
    (attention): Attention(
      (encoder_att): Linear(in_features=512, out_features=256, bias=True)
      (decoder_att): Linear(in_features=512, out_features=256, bias=True)
      (full_att): Linear(in_features=256, out_features=1, bias=True)
      (relu): ReLU()
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.1, inplace=False)
      (attention_gate): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=1, bias=True)
      )
    )
    (embedding): Embedding(417, 128)
    (dropout_layer): Dropout(p=0.1, inplace=False)
    (gru1): GRUCell(640, 512)
    (gru2): GRUCell(512, 512)
    (gru3): GRUCell(512, 512)
    (input_projection): Linear(in_features=640, out_features=512, bias=True)
    (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (fc): Linear(in_features=512, out_features=417, bias=True)
  )
)
